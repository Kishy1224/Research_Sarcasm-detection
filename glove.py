# -*- coding: utf-8 -*-
"""Glove

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Prk90NhGmwiR8s1w7VzkDm8EFe8ny-Ck
"""

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import tensorflow as tf
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

import tensorflow as tf
import sklearn
import re
import codecs
from tqdm import tqdm
from wordcloud import WordCloud,STOPWORDS

pd.set_option('display.width', 1000)

from google.colab import drive 
drive.mount('/content/gdrive')

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

df = pd.read_csv("/content/gdrive/MyDrive/Colab Notebooks/NLP projects /sarcasm.csv")

print(df.shape)
df.head()

df.rename(columns = {'headline':'text', 'is_sarcastic':'target'}, inplace = True)

df.head()

df.drop('Unnamed: 0',axis = 1,inplace=True)

df.describe()

df_preprocess = df.copy()
sarc = df_preprocess.text.copy()

sarc

def clean_text (text):
     text = text.lower ()
     text = re.sub (r"[^  sa-zA-Z0-9 @  []]", '', text) # Removes punctuation #removes character for bracket one item
     text = re.sub (r"w *  d +  w *", '', text) # Remove digits
     text = re.sub (' s {2,}', "", text) # Removes unnecessary spaces
     
     return text

sarc_clean = []
for t in sarc:
    sarc_clean.append(clean_text(t))

corpus = []
for text in sarc_clean:
    corpus.append(text.replace("'","").replace(",","").replace("[","").replace("]",""))

corpus

len(corpus)

def tokenize(texts):
    tokenizer = nltk.RegexpTokenizer(r'\w+')

    texts_tokens = []
    for i, val in enumerate(texts):
        text_tokens = tokenizer.tokenize(val.lower())

        for i in range(len(text_tokens) - 1, -1, -1): #tokens in reverse order
            if len(text_tokens[i]) < 4:
                del(text_tokens[i])

        texts_tokens.append(text_tokens)
        
    return texts_tokens

sarc_tokens = tokenize(corpus)
sarc_tokens

def removeSW(texts_tokens):
    stopWords = set(stopwords.words('english'))
    texts_filtered = []

    for i, val in enumerate(texts_tokens):
        text_filtered = []
        for w in val:
            if w not in stopWords:
                text_filtered.append(w)
        texts_filtered.append(text_filtered)
        
    return texts_filtered

Sarc_filtered = removeSW(sarc_tokens)

Sarc_filtered[4]

def lemma(texts_filtered):
    wordnet_lemmatizer = WordNetLemmatizer()
    texts_lem = []

    for i, val in enumerate(texts_filtered):
        text_lem = []
        for word in val:
            text_lem.append(wordnet_lemmatizer.lemmatize(word, pos="v")) #verb parts of speech
        texts_lem.append(text_lem)
    
    return texts_lem

Sarc_lem = lemma(Sarc_filtered)

Sarc_lem[4]

Sarc_ready = []
for sarcs in Sarc_lem:
    string = ' '
    string = string.join(sarcs)
    Sarc_ready.append(string)

Sarc_ready

df_preprocess['Sarcasm'] = Sarc_ready

df_preprocess.head()

old_cols = df_preprocess.columns.values 
new_cols= ['Sarcasm', 'text', 'target']
df_preprocess = df_preprocess.reindex(columns=new_cols)

plt.figure(figsize = (20,20)) # Text that is not Fake
wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(" ".join(df[df.target == 1].text))
plt.imshow(wc , interpolation = 'bilinear')

plt.figure(figsize = (20,20)) # Text that is Fake
wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(" ".join(df[df.target == 0].text))
plt.imshow(wc , interpolation = 'bilinear')

from sklearn.model_selection import  train_test_split
X_train, X_test, y_train, y_test = train_test_split(df_preprocess['Sarcasm'], df_preprocess['target'], train_size = 0.6,test_size=0.4, random_state=8)
X_test,X_val,y_test,y_val = train_test_split(X_test,y_test, test_size=0.5, random_state=123)
print("Train Data size:", len(X_train))
print("Test Data size", len(X_test))
print("val Data size", len(X_val))

max_features = 19300
maxlen = 30

from keras.preprocessing import text, sequence

tokenizer = text.Tokenizer(num_words=max_features)
tokenizer.fit_on_texts(X_train)
tokenized_train = tokenizer.texts_to_sequences(X_train)
X_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)

tokenized_test = tokenizer.texts_to_sequences(X_test)
X_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)

tokenized_val = tokenizer.texts_to_sequences(X_val)
X_val = sequence.pad_sequences(tokenized_val, maxlen=maxlen)

EMBEDDING_FILE = '/content/gdrive/MyDrive/Colab Notebooks/NLP/glove.6B.200d.txt'

def get_coefs(word, *arr): 
    return word, np.asarray(arr, dtype='float32')
embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))

len(embeddings_index)

all_embs = np.stack(embeddings_index.values())
emb_mean,emb_std = all_embs.mean(), all_embs.std()
embed_size = all_embs.shape[1]

word_index = tokenizer.word_index
nb_words = min(max_features, len(word_index))
#change below line if computing normal stats is too slow
embedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))
for word, i in word_index.items():
    if i >= max_features: continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None: embedding_matrix[i] = embedding_vector

vocabulary_size = len(tokenizer.word_index)
print("Vocabulary Size :", vocabulary_size)

batch_size = 256
epochs = 10
embed_size = 200

from keras.models import Sequential
from keras.layers import Dense,Embedding,LSTM,Dropout
from keras.callbacks import ReduceLROnPlateau
import tensorflow as tf

learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)

len(embedding_matrix)

MAX_SEQUENCE_LENGTH = 30

embedding_layer = tf.keras.layers.Embedding(vocabulary_size,
                                          embed_size,
                                          weights=[embedding_matrix],
                                          input_length=MAX_SEQUENCE_LENGTH,
                                          trainable=False)

from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout, SpatialDropout1D, GlobalMaxPooling1D
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
from tensorflow.keras.layers import RNN

"""*Model training*

## CNN+Bi-LSTM
## stacked Bi-lstm he uniform weight initializer
## GRU
## GRU + BI-LSTM
## Bi-LSTM with glorot uniform weight initializer
"""

inputs = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype="int32")

x = embedding_layer(inputs)
x = SpatialDropout1D(0.2)(x)  

# Conv1D + LSTM (bidirectional)
x = Conv1D(64, 5, activation='relu')(x)
x = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)

x = Dense(512, activation="relu")(x)
x = Dropout(0.5)(x)

outputs = Dense(1, activation="sigmoid")(x)

model_B = tf.keras.Model(inputs, outputs)

model_B.compile(optimizer=Adam(lr = 0.01), loss='binary_crossentropy',
              metrics=['accuracy'])

history = model_B.fit(X_train, y_train, batch_size = batch_size , validation_data = (X_val,y_val) , epochs = epochs , callbacks = [learning_rate_reduction])

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.ylabel('Accuracy')
plt.title('Conv + BiLstm with word embedding fasttext')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.ylabel('loss')
plt.title('Conv + BiLstm with word embedding fasttext')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

score = model_B.evaluate(X_test, y_test, verbose = 0)

score

y_pred = model_B.predict(X_test)

print('Test loss:', score[0]*100,'%') 
print('Test accuracy:', score[1]*100,'%')

def predict_sarc(sent):
  pass

"""Stacked LSTM"""

Inputs = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype="int32")

x = embedding_layer(Inputs)
x = SpatialDropout1D(0.2)(x)

x = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2,return_sequences=True))(x)
x = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2,return_sequences=True))(x)

# Vanilla hidden layer:
x = Dense(512,kernel_initializer='he_uniform',activation='tanh')(x)

x = Dropout(0.5)(x)

Outputs = Dense(1, activation="sigmoid")(x)

model_C = tf.keras.Model(Inputs, Outputs)

model_C.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy',
              metrics=['accuracy'])

history_C = model_C.fit(X_train, y_train, batch_size = batch_size , validation_data = (X_val,y_val) , epochs = epochs , callbacks = [learning_rate_reduction])

plt.plot(history_C.history['accuracy'])
plt.plot(history_C.history['val_accuracy'])
plt.ylabel('Accuracy')
plt.title('Stacked LStM with word embedding fasttext')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.plot(history_C.history['loss'])
plt.plot(history_C.history['val_loss'])
plt.ylabel('loss')
plt.title('Stacked BiLstm with word embedding fasttext')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

score1 = model_C.evaluate(X_val, y_val, verbose = 0)

print('Test loss:', score1[0]*100,'%') 
print('Test accuracy:', score1[1]*100,'%')

"""Gated Recurrent Unit"""

inp = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype="int32")

x = embedding_layer(inp)
x = tf.keras.layers.GRU(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(x)

x = GlobalMaxPooling1D()(x)
x = Dense(256, activation="relu")(x)
x = Dropout(0.2)(x)

outp = Dense(1, activation="sigmoid")(x)

model_D = tf.keras.Model(inp, outp)

model_D.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy',
              metrics=['accuracy'])

history_D = model_D.fit(X_train, y_train, batch_size = batch_size , validation_data = (X_val,y_val) , epochs = 30 , callbacks = [learning_rate_reduction])

score2 = model_D.evaluate(X_test, y_test, verbose = 0) 
score2

plt.plot(history_D.history['accuracy'])
plt.plot(history_D.history['val_accuracy'])
plt.ylabel('Accuracy')
plt.title('GRU with word embedding fasttext')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.plot(history_D.history['loss'])
plt.plot(history_D.history['val_loss'])
plt.ylabel('loss')
plt.title('GRU with word embedding fasttext')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

"""GRU + BI-LSTM"""

IN = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype="int32")

x = embedding_layer(IN)
x = tf.keras.layers.GRU(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(x)

x = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)
x = Dense(256, activation="relu")(x)
x = Dropout(0.2)(x)

OUT = Dense(1, activation="sigmoid")(x)

model_E = tf.keras.Model(IN, OUT)

model_E.compile(optimizer=Adam(), loss='binary_crossentropy',
              metrics=['accuracy'])

history_E = model_E.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=25,
                    validation_data=(X_test, y_test), callbacks=[es, reduce_lr])

score3 = model_E.evaluate(X_test, y_test, verbose = 0) 
score3

plt.plot(history_E.history['accuracy'])
plt.plot(history_E.history['val_accuracy'])
plt.ylabel('Accuracy')
plt.title('GRU+BI-lstm with word embedding fasttext')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.plot(history_E.history['loss'])
plt.plot(history_E.history['val_loss'])
plt.ylabel('loss')
plt.title('GRU+BI-lstm with word embedding fasttext')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

inner = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype="int32")

x = embedding_layer(inner)
x = SpatialDropout1D(0.2)(x)  

# Conv1D + LSTM (bidirectional)
x = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2))(x)

x = Dense(512, kernel_initializer = 'glorot_uniform',activation="relu")(x)
x = Dropout(0.5)(x)

outer = Dense(1, activation="sigmoid")(x)

model_F = tf.keras.Model(inner, outer)

model_F.compile(optimizer=Adam(), loss='binary_crossentropy',
              metrics=['accuracy'])

history_F = model_F.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=25,
                    validation_data=(X_val, y_val), callbacks=[es, reduce_lr])

score4 = model_F.evaluate(X_test, y_test, verbose = 0) 
score4

plt.plot(history_F.history['accuracy'])
plt.plot(history_F.history['val_accuracy'])
plt.ylabel('Accuracy')
plt.title('BI-lstm with word embedding fasttext')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.plot(history_F.history['loss'])
plt.plot(history_F.history['val_loss'])
plt.ylabel('loss')
plt.title('BI-lstm with word embedding fasttext')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')