# -*- coding: utf-8 -*-
"""sar_3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YQ1hFk_PfICO_RM34VOmVXF_fi_cU8el
"""

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

import tensorflow as tf
import sklearn
import re
import codecs
from tqdm import tqdm

pd.set_option('display.width', 1000)

from google.colab import drive 
drive.mount('/content/gdrive')

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

df = pd.read_csv("/content/gdrive/MyDrive/Colab Notebooks/NLP projects /sarcasm.csv")

print(df.shape)
df.head()

df.rename(columns = {'headline':'text', 'is_sarcastic':'target'}, inplace = True)

df.head()

df.drop('Unnamed: 0',axis = 1,inplace=True)

df.describe()

df_preprocess = df.copy()
sarc = df_preprocess.text.copy()

sarc

def clean_text (text):
     text = text.lower ()
     text = re.sub (r"[^  sa-zA-Z0-9 @  []]", '', text) # Removes punctuation #removes character for bracket one item
     text = re.sub (r"w *  d +  w *", '', text) # Remove digits
     text = re.sub (' s {2,}', "", text) # Removes unnecessary spaces
     
     return text

sarc_clean = []
for t in sarc:
    sarc_clean.append(clean_text(t))

corpus = []
for text in sarc_clean:
    corpus.append(text.replace("'","").replace(",","").replace("[","").replace("]",""))

corpus

len(corpus)

def tokenize(texts):
    tokenizer = nltk.RegexpTokenizer(r'\w+')

    texts_tokens = []
    for i, val in enumerate(texts):
        text_tokens = tokenizer.tokenize(val.lower())

        for i in range(len(text_tokens) - 1, -1, -1): #tokens in reverse order
            if len(text_tokens[i]) < 4:
                del(text_tokens[i])

        texts_tokens.append(text_tokens)
        
    return texts_tokens

sarc_tokens = tokenize(corpus)
sarc_tokens

def removeSW(texts_tokens):
    stopWords = set(stopwords.words('english'))
    texts_filtered = []

    for i, val in enumerate(texts_tokens):
        text_filtered = []
        for w in val:
            if w not in stopWords:
                text_filtered.append(w)
        texts_filtered.append(text_filtered)
        
    return texts_filtered

Sarc_filtered = removeSW(sarc_tokens)

Sarc_filtered[4]

def lemma(texts_filtered):
    wordnet_lemmatizer = WordNetLemmatizer()
    texts_lem = []

    for i, val in enumerate(texts_filtered):
        text_lem = []
        for word in val:
            text_lem.append(wordnet_lemmatizer.lemmatize(word, pos="v")) #verb parts of speech
        texts_lem.append(text_lem)
    
    return texts_lem

Sarc_lem = lemma(Sarc_filtered)

Sarc_lem[4]

Sarc_ready = []
for sarcs in Sarc_lem:
    string = ' '
    string = string.join(sarcs)
    Sarc_ready.append(string)

Sarc_ready

df_preprocess['Sarcasm'] = Sarc_ready

df_preprocess.head()

old_cols = df_preprocess.columns.values 
new_cols= ['Sarcasm', 'text', 'target']
df_preprocess = df_preprocess.reindex(columns=new_cols)

import codecs   #Fastext
from tqdm import tqdm
embeddings_index = {}
f = codecs.open('/content/gdrive/MyDrive/Colab Notebooks/NLP projects /wiki.simple.vec', encoding='utf-8')
for line in tqdm(f):
    values = line.rstrip().rsplit(' ')
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
print('found %s word vectors' % len(embeddings_index))

EMBEDDING_DIM = 300
from keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(df_preprocess.Sarcasm)

word_index = tokenizer.word_index
vocabulary_size = len(tokenizer.word_index) + 1
print("Vocabulary Size :", vocabulary_size)

word_index.items()
#embeddings_index.get('trump')

embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))
for word, i in word_index.items():
  embedding_vector = embeddings_index.get(word)
  if embedding_vector is not None:
    embedding_matrix[i] = embedding_vector

embedding_matrix

MAX_SEQUENCE_LENGTH = 30

embedding_layer = tf.keras.layers.Embedding(vocabulary_size,
                                          EMBEDDING_DIM,
                                          weights=[embedding_matrix],
                                          input_length=MAX_SEQUENCE_LENGTH,
                                          trainable=False)

from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout, SpatialDropout1D, GlobalMaxPooling1D
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
from tensorflow.keras.layers import RNN

"""## CNN+Bi-LSTM
## stacked Bi-lstm he uniform weight initializer
## GRU
## GRU + BI-LSTM
## Bi-LSTM with glorot uniform weight initializer
"""

inputs = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype="int32")

x = embedding_layer(inputs)
x = SpatialDropout1D(0.2)(x)  

# Conv1D + LSTM (bidirectional)
x = Conv1D(64, 5, activation='relu')(x)
x = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)

x = Dense(512, activation="relu")(x)
x = Dropout(0.5)(x)

outputs = Dense(1, activation="sigmoid")(x)

model_B = tf.keras.Model(inputs, outputs)

model_B.compile(optimizer=Adam(), loss='binary_crossentropy',
              metrics=['accuracy'])

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df_preprocess['Sarcasm'], df_preprocess['target'], test_size=0.2, random_state=123)
#X_train, X_val, y_train, y_val = train_test_split(df_preprocess['Sarcasm'], df_preprocess['target'], test_size=0.2, random_state=123)
print("Train Data size:", len(X_train))
print("Test Data size", len(X_test))

BATCH_SIZE = 1024
EPOCHS = 30

from keras.preprocessing.sequence import pad_sequences

X_train = pad_sequences(tokenizer.texts_to_sequences(X_train),
                        maxlen = MAX_SEQUENCE_LENGTH)
X_test = pad_sequences(tokenizer.texts_to_sequences(X_test),
                       maxlen = MAX_SEQUENCE_LENGTH)

print("Training X Shape:",X_train.shape)
print("Testing X Shape:",X_test.shape)

es=EarlyStopping(monitor='val_loss',
                 mode='min',
                 verbose=1,
                 patience=5)

reduce_lr = ReduceLROnPlateau(factor=0.1,
                                     min_lr = 0.02,
                                     monitor = 'val_loss',
                                     verbose = 1)

history_B = model_B.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,
                    validation_data=(X_test, y_test), callbacks=[es, reduce_lr])

plt.plot(history_B.history['accuracy'])
plt.plot(history_B.history['val_accuracy'])
plt.ylabel('Accuracy')
plt.title('Conv + BiLstm with word embedding fasttext')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.plot(history_B.history['loss'])
plt.plot(history_B.history['val_loss'])
plt.ylabel('loss')
plt.title('Conv + BiLstm with word embedding fasttext')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

score = model_B.evaluate(X_test, y_test, verbose = 0)

y_pred = model_B.predict(X_test)

print('Test loss:', score[0]*100,'%') 
print('Test accuracy:', score[1]*100,'%')

def predict_sarc(sent):
  pass

"""Stacked LSTM"""

Inputs = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype="int32")

x = embedding_layer(Inputs)
x = SpatialDropout1D(0.2)(x)

x = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2,return_sequences=True))(x)
x = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2,return_sequences=True))(x)

# Vanilla hidden layer:
x = Dense(512,kernel_initializer='he_uniform',activation='tanh')(x)

x = Dropout(0.5)(x)

Outputs = Dense(1, activation="sigmoid")(x)

model_C = tf.keras.Model(Inputs, Outputs)

model_C.compile(optimizer=Adam(), loss='binary_crossentropy',
              metrics=['accuracy'])

history_C = model_C.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,
                    validation_data=(X_test, y_test), callbacks=[es, reduce_lr])

plt.plot(history_C.history['accuracy'])
plt.plot(history_C.history['val_accuracy'])
plt.ylabel('Accuracy')
plt.title('Stacked LStM with word embedding fasttext')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.plot(history_C.history['loss'])
plt.plot(history_C.history['val_loss'])
plt.ylabel('loss')
plt.title('Stacked BiLstm with word embedding fasttext')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

score1 = model_C.evaluate(X_test, y_test, verbose = 0)

print('Test loss:', score1[0]*100,'%') 
print('Test accuracy:', score1[1]*100,'%')

"""Gated Recurrent Unit"""

inp = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype="int32")

x = embedding_layer(inp)
x = tf.keras.layers.GRU(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(x)

x = GlobalMaxPooling1D()(x)
x = Dense(256, activation="relu")(x)
x = Dropout(0.2)(x)

outp = Dense(1, activation="sigmoid")(x)

model_D = tf.keras.Model(inp, outp)

model_D.compile(optimizer=Adam(), loss='binary_crossentropy',
              metrics=['accuracy'])

history_D = model_D.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,
                    validation_data=(X_test, y_test), callbacks=[es, reduce_lr])

score2 = model_D.evaluate(X_test, y_test, verbose = 0) 
score2

plt.plot(history_D.history['accuracy'])
plt.plot(history_D.history['val_accuracy'])
plt.ylabel('Accuracy')
plt.title('GRU with word embedding fasttext')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.plot(history_D.history['loss'])
plt.plot(history_D.history['val_loss'])
plt.ylabel('loss')
plt.title('GRU with word embedding fasttext')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

"""GRU + BI-LSTM"""

IN = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype="int32")

x = embedding_layer(IN)
x = tf.keras.layers.GRU(32, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(x)

x = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)
x = Dense(256, activation="relu")(x)
x = Dropout(0.2)(x)

OUT = Dense(1, activation="sigmoid")(x)

model_E = tf.keras.Model(IN, OUT)

model_E.compile(optimizer=Adam(), loss='binary_crossentropy',
              metrics=['accuracy'])

history_E = model_E.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=25,
                    validation_data=(X_test, y_test), callbacks=[es, reduce_lr])

score3 = model_E.evaluate(X_test, y_test, verbose = 0) 
score3

plt.plot(history_E.history['accuracy'])
plt.plot(history_E.history['val_accuracy'])
plt.ylabel('Accuracy')
plt.title('GRU+BI-lstm with word embedding fasttext')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.plot(history_E.history['loss'])
plt.plot(history_E.history['val_loss'])
plt.ylabel('loss')
plt.title('GRU+BI-lstm with word embedding fasttext')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

inner = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype="int32")

x = embedding_layer(inner)
x = SpatialDropout1D(0.2)(x)  

# Conv1D + LSTM (bidirectional)
x = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2))(x)

x = Dense(512, kernel_initializer = 'glorot_uniform',activation="relu")(x)
x = Dropout(0.5)(x)

outer = Dense(1, activation="sigmoid")(x)

model_F = tf.keras.Model(inner, outer)

model_F.compile(optimizer=Adam(), loss='binary_crossentropy',
              metrics=['accuracy'])

history_F = model_F.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=25,
                    validation_data=(X_test, y_test), callbacks=[es, reduce_lr])

score4 = model_F.evaluate(X_test, y_test, verbose = 0) 
score4

plt.plot(history_F.history['accuracy'])
plt.plot(history_F.history['val_accuracy'])
plt.ylabel('Accuracy')
plt.title('BI-lstm with word embedding fasttext')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.plot(history_F.history['loss'])
plt.plot(history_F.history['val_loss'])
plt.ylabel('loss')
plt.title('BI-lstm with word embedding fasttext')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')